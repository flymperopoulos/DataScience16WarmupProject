{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Iteration Part 2 - Exploring Better Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got super excited from the reading on improving the submission of our first model exploration. Random forests and, more precisely, featuring seem very cool methods that I want to implement to improve my model. I am initially going to import all the relevant libraries and load the corresponding csv file with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "titanic = pandas.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we will substitute all null/NaN values with the median of the column/parameter we are interested in playing with; this time, \"Age\" and \"Embarked\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "# Replace all the occurences with the respctive value.\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == 'S', \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == 'C', \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == 'Q', \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to protect our model from overfitting, it is fundamental to break down the data into three folds, while at the same time, using the Linear Regression library to play with the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Algorith and folding declaration\n",
    "alg = LinearRegression()\n",
    "kf = sklearn.cross_validation.KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_predictors = (titanic[predictors].iloc[train,:])\n",
    "    \n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    \n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the different folds back together, we get the following. In essence, it amalgamates the predictions in a binary fashion, by assigning any prediciton below and, including 0.5, to 0 and everything else to 1. Counting the number of 1's that appeared and dividing that over the total number of predictions yields an accuracy that is not super great but not bad either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of this model is just 0.783389450056\n"
     ]
    }
   ],
   "source": [
    "predictions = np.concatenate(predictions, axis=0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "\n",
    "survivedCounter = 0.0\n",
    "\n",
    "for i in range(0,len(predictions)):\n",
    "     if (predictions[i] == titanic[\"Survived\"][i]):\n",
    "        survivedCounter += 1\n",
    "        \n",
    "accuracy = survivedCounter / len(predictions)\n",
    "print \"The accuracy of this model is just\", accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of actually combining all the folds together and then looping through and counting the binary positive outcomes, one could apply Linear Regression to each different fold \"bucket\" and then average the predictions that each of the folds yielded, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new accuracy with linear regression on the three independent folds is averaged out to be 0.787878787879\n"
     ]
    }
   ],
   "source": [
    "alg = sklearn.linear_model.LogisticRegression(random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print \"The new accuracy with linear regression on the three independent folds is averaged out to be\", scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly, I am not super interested in submitting this result to Kaggle and applying it to the test data, so how about we procceed to the development of a different model that will hopeully give us a better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I previously mentioned, I really liked the way the DataQuest exploration combined random forest with featuring and I would like to apply such an algorithmic approach in my model. As seen below, I am applying a random forest and calculating the mean score out of the three folds we have. By tweaking the n_estimators and the min_samples_split parameters we observe an increase in the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediciton score has improved to 0.826038159371\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=5, min_samples_leaf=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "\n",
    "print \"The prediciton score has improved to\", scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now apply this model to our test data and make a submission to kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic_test = pandas.read_csv(\"test.csv\")\n",
    "\n",
    "# Handle NaNs\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic_test[\"Fare\"].median())\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == 'male', \"Sex\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == 'female', \"Sex\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == 'S', \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == 'C', \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == 'Q', \"Embarked\"] = 2\n",
    "\n",
    "\n",
    "alg = sklearn.linear_model.LogisticRegression(random_state=1)\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"kaggleForest.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I am going to explore a final version of this model search in order to improve my model. Something that I found really interesting was the ability to determine which fields in the data are the most important to train out of. Hence, let's write a script that indicates which are the preditors that we should be using the most in our new revised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass:  24.5956714208\n",
      "Sex:  68.8519942529\n",
      "Age:  1.27768954597\n",
      "SibSp:  0.534254502442\n",
      "Parch:  1.82976042906\n",
      "Fare:  14.2132351418\n",
      "Embarked:  2.85130099045\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEpCAYAAABYyHNYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npgtv3gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1098e7910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "for i in range(0, len(predictors)):\n",
    "    print predictors[i] + ': ', scores[i]\n",
    "\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the graph and value generated above, we can proceed as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.818181818182\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print \"Accuracy\", scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above is pretty much the same with the one we deduced earlier, therefore, we are going to develop a set of new features that we hope can aid us in the improving of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating a familysize column\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Col           2\n",
      "Major         2\n",
      "Mlle          2\n",
      "Countess      1\n",
      "Ms            1\n",
      "Lady          1\n",
      "Jonkheer      1\n",
      "Don           1\n",
      "Mme           1\n",
      "Capt          1\n",
      "Sir           1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything.\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1      800\n",
      " 14       8\n",
      " 149      7\n",
      " 63       6\n",
      " 50       6\n",
      " 59       6\n",
      " 17       5\n",
      " 384      4\n",
      " 27       4\n",
      " 25       4\n",
      " 162      4\n",
      " 8        4\n",
      " 84       4\n",
      " 340      4\n",
      " 43       3\n",
      " 269      3\n",
      " 58       3\n",
      " 633      2\n",
      " 167      2\n",
      " 280      2\n",
      " 510      2\n",
      " 90       2\n",
      " 83       1\n",
      " 625      1\n",
      " 376      1\n",
      " 449      1\n",
      " 498      1\n",
      " 588      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# A dictionary mapping family name to id\n",
    "family_id_mapping = {}\n",
    "\n",
    "# A function to get the id given a row\n",
    "def get_family_id(row):\n",
    "    # Find the last name by splitting on a comma\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    # Create the family id\n",
    "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "    # Look up the id in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id] = current_id\n",
    "    return family_id_mapping[family_id]\n",
    "\n",
    "# Get the family ids with the apply method\n",
    "family_ids = titanic.apply(get_family_id, axis=1)\n",
    "\n",
    "# There are a lot of family ids, so we'll compress all of the families under 3 members into one code.\n",
    "family_ids[titanic[\"FamilySize\"] < 3] = -1\n",
    "\n",
    "# Print the count of each unique id.\n",
    "print(pandas.value_counts(family_ids))\n",
    "\n",
    "titanic[\"FamilyId\"] = family_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEsCAYAAAAIBeLrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d0ca6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811447811448\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly enough, the \"Title\" parameter is significant in the exploration of an accurate surviral rate prediction, along with Sex and PClass while the parameter of FAmilySize and FamilyId doesn't seem to influence the result as much. Time to predict on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     240\n",
      "2      79\n",
      "3      72\n",
      "4      21\n",
      "7       2\n",
      "6       2\n",
      "10      1\n",
      "5       1\n",
      "Name: Title, dtype: int64\n",
      "{\"O'Sullivan0\": 426, 'Mangan0': 620, 'Lindqvist1': 543, 'Denkoff0': 297, 'Rouse0': 413, 'Berglund0': 207, 'Meo0': 142, 'Arnold-Franchi1': 49, 'Chronopoulos1': 71, 'Skoog5': 63, 'Widener2': 329, 'Pengelly0': 217, 'Goncalves0': 400, 'Myhrman0': 626, 'Beane1': 456, 'Moss0': 104, 'Carlsson0': 610, 'Nicholls2': 136, 'Jussila1': 110, 'Jussila0': 483, 'Long0': 632, 'Wheadon0': 33, 'Connolly0': 261, 'Hansen2': 680, 'Stephenson1': 493, 'Davies0': 336, 'Silven2': 359, 'Vanden Steen0': 311, 'Astor1': 571, 'Patchett0': 480, 'Johanson0': 184, 'Coleridge0': 220, 'Christmann0': 87, 'Carter3': 340, 'Compton2': 665, 'Carter1': 226, 'Turkula0': 414, 'Hassab0': 558, 'Saad0': 566, 'Mellors0': 208, 'Mamee0': 36, 'Madsen0': 119, 'Anderson0': 395, 'Kraeff0': 42, 'Robbins0': 468, 'Lundahl0': 522, 'Gilinski0': 490, 'Porter0': 107, 'Sdycoff0': 352, 'Green0': 204, 'Bishop1': 263, 'Sinkkonen0': 603, 'Otter0': 640, 'Dahl0': 299, 'Troutt0': 582, 'Samaan2': 48, 'Edvardsson0': 553, 'Petroff0': 98, 'Burke0': 134, 'Cardeza1': 556, 'Hawksford0': 599, 'Somerton0': 416, 'Healy0': 248, 'Andersson0': 137, 'Fortune5': 27, 'Andersson6': 14, 'Johnson2': 9, 'Johnson0': 273, 'Coxon0': 91, 'Banfield0': 696, 'McCarthy0': 7, 'Panula5': 50, 'West3': 58, 'Kallio0': 374, 'Hoyt1': 206, 'Hoyt0': 638, 'Mannion0': 589, 'Touma2': 232, 'Futrelle1': 4, 'Jardin0': 507, 'Lemore0': 437, 'Davies2': 460, 'Robert1': 630, 'Butt0': 451, 'Wick2': 286, 'Emanuel0': 628, 'Ibrahim Shawah0': 643, 'Carbines0': 175, 'McEvoy0': 583, 'Moutal0': 75, 'Ling0': 157, 'Watson0': 552, 'Lahoud0': 443, 'Sedgwick0': 302, 'Beesley0': 22, 'Sheerlinck0': 79, 'Hunt0': 218, 'Clifford0': 408, 'Stranden0': 602, 'Ahlin1': 40, 'Woolner0': 55, 'Caram1': 482, 'Sandstrom2': 11, 'Nakid2': 333, 'Frost0': 412, 'Moen0': 73, 'Henry0': 239, 'Persson1': 241, 'Mudd0': 671, 'Glynn0': 32, 'Bowerman1': 312, 'Daniel0': 505, 'Nysten0': 132, 'Saalfeld0': 270, 'Turcin0': 173, 'Nasser1': 10, 'Waelens0': 78, 'Homer0': 502, 'Graham1': 242, 'Graham0': 699, 'Brown0': 177, 'Dorking0': 256, 'Bing0': 72, 'Blank0': 191, 'Tornquist0': 245, 'Olsen1': 180, 'Olsen0': 144, 'Davison1': 304, 'Butler0': 544, 'Calderhead0': 575, 'Cor0': 530, 'Kalvik0': 534, 'Hagland1': 386, 'Boulos2': 131, 'Boulos0': 497, 'Leitch0': 496, 'Endres0': 581, 'Fleming0': 275, 'Natsch1': 247, 'Garfirth0': 614, 'Blackwell0': 300, 'Thayer2': 461, 'Holm0': 657, 'Rekic0': 105, 'Allen0': 5, 'Hendekovic0': 282, 'Svensson0': 424, 'Montvila0': 698, 'Perreault0': 441, 'Francatelli0': 278, 'Gronnestad0': 621, 'Maisner0': 399, 'Radeff0': 537, 'Daly0': 432, 'Jarvis0': 488, 'Bowen0': 515, 'Risien0': 453, 'Walker0': 436, 'Klaber0': 578, 'Wright0': 466, 'Artagaveytia0': 419, 'Navratil2': 138, 'Hays2': 658, 'Seward0': 383, 'Hays0': 279, 'Appleton2': 478, 'Fry0': 654, 'Romaine0': 171, 'de Messemaeker1': 469, 'Gustafsson0': 331, 'Gustafsson2': 101, 'Lulic0': 659, 'Beckwith2': 225, 'Sutton0': 516, 'Cunningham0': 355, 'Behr0': 700, 'Rothschild1': 434, 'Stankovic0': 257, 'Abelson1': 277, 'Lobb1': 230, 'Shellard0': 423, 'Knight0': 593, 'Silverthorne0': 572, \"O'Leary0\": 535, 'Saundercock0': 13, 'Baxter1': 114, 'Elias0': 624, 'Nankoff0': 598, 'Mitkoff0': 533, 'Lines1': 677, 'Burns0': 298, \"O'Driscoll0\": 47, 'Cumings1': 2, 'Culumovic0': 674, 'Penasco y Castellana1': 276, 'Parkes0': 251, 'Duff Gordon1': 467, 'Sloper0': 24, 'Strom2': 228, 'Ringhini0': 327, 'Barah0': 616, 'Richards2': 350, 'Crease0': 67, 'Cavendish1': 600, 'Bjornstrom-Steffansson0': 371, 'Masselmani0': 20, 'Moubarek2': 65, 'Turja0': 555, 'Goldsmith2': 154, 'Fahlstrom0': 210, 'Holverson1': 35, 'Herman3': 510, 'Nenkoff0': 205, 'Nysveen0': 292, 'Eklund0': 617, 'Paulner0': 487, 'Nilsson0': 284, 'Dick1': 563, 'Dantcheff0': 639, 'Swift0': 682, 'Humblen0': 570, 'McCormack0': 661, 'Duane0': 253, 'Ekstrom0': 121, 'Wiseman0': 366, 'Smith0': 160, 'Marvin1': 604, 'Cleaver0': 576, 'Gill0': 683, 'Cameron0': 193, 'Augustsson0': 663, 'Petranec0': 97, 'Stahelin-Maeglin0': 523, 'McDermott0': 80, 'Dean3': 90, 'Laleff0': 691, 'Meyer0': 649, 'Meyer1': 34, 'Johansson0': 100, 'Cacic0': 405, 'Ryerson4': 280, 'Becker3': 167, 'Tobin0': 627, 'Murphy1': 219, 'Goldschmidt0': 93, 'Kink2': 68, 'Weisz1': 125, 'Hold1': 215, 'Stanley0': 420, 'Minahan2': 222, 'Toufik0': 450, 'Minahan1': 354, 'Olsvigen0': 559, 'Odahl0': 307, 'Coleff0': 435, 'Vestrom0': 15, 'Hood0': 70, 'Yousseff0': 421, 'Barton0': 109, 'Icard0': 61, 'Smiljanic0': 148, 'Harmer0': 634, 'Ford4': 84, 'Maioni0': 428, 'Abbing0': 675, 'Oreskovic0': 347, 'Sundman0': 356, 'Kink-Heilmann2': 168, 'Lehmann0': 339, 'Dennis0': 288, 'del Carlo1': 316, 'Najib0': 690, 'Ohman0': 465, 'McCoy2': 272, 'Osen0': 129, 'Pernot0': 166, 'Kent0': 415, 'Turpin1': 41, 'Zabour1': 108, 'Alexander0': 650, 'Palsson4': 8, 'Hogeboom1': 618, 'Keefe0': 404, 'Eustis1': 422, 'Marechal0': 669, 'Uruchurtu0': 30, 'Downton0': 485, 'Ilmakangas1': 591, 'Corn0': 147, 'Parrish1': 236, 'Jerwan0': 406, 'Aks1': 678, 'Lam0': 565, 'Slayter0': 290, 'Weir0': 567, 'Frolicher2': 454, 'Danbom2': 365, 'Alhomaki0': 670, 'Brown2': 548, 'Faunthorpe1': 53, 'Givard0': 195, 'Leyson0': 213, 'Potter1': 692, 'Emir0': 26, 'Slocovski0': 85, 'Celotti0': 86, 'Rood0': 169, 'Betros0': 330, 'Larsson0': 211, 'Andreasson0': 88, 'Vande Walle0': 183, 'Morley0': 396, 'Trout0': 344, 'Thorneycroft1': 372, 'Shorney0': 92, 'Attalah0': 111, 'Ward0': 235, 'Yousif0': 310, 'Wiklund1': 325, 'Thorne0': 233, 'Warren1': 320, 'Milling0': 398, 'Rommetvedt0': 545, 'Pickard0': 370, 'Hassan0': 592, 'Heininen0': 655, 'Bazzani0': 200, 'Plotcharsky0': 335, 'Lindell1': 503, 'Caldwell2': 76, 'Meanwell0': 474, 'Bystrom0': 684, 'Harrison0': 238, 'Goldenberg1': 388, 'Webber0': 117, 'Shelley1': 693, 'Mayne0': 577, 'Honkanen0': 198, 'Karlsson0': 410, 'Calic0': 153, 'Eitemiller0': 538, 'Chapman0': 568, 'Asplund6': 25, 'Peuchen0': 385, 'Cairns0': 244, 'Bailey0': 611, 'Hanna0': 268, 'Garside0': 481, 'Mernagh0': 179, 'Staneff0': 74, 'Toomey0': 393, 'Canavan0': 425, 'Dakic0': 560, 'Foo0': 529, 'Pasic0': 666, 'Novel0': 57, 'Lemberopolous0': 673, 'Hocking4': 625, 'Moran1': 106, 'Abbott2': 252, 'Kvillner0': 377, 'Elias2': 309, 'Doharr0': 476, 'Norman0': 472, 'Leader0': 641, 'Smart0': 402, 'White1': 99, 'Gale1': 348, 'Doling1': 95, 'Moor1': 607, 'Taussig2': 237, 'Pinsky0': 174, 'Gallagher0': 573, 'Markun0': 694, 'de Mulder0': 258, 'Kassem0': 444, 'Yrois0': 182, 'Kantor1': 96, 'Sobey0': 126, 'Shutes0': 506, 'Brocklebank0': 509, 'Cherry0': 234, 'Dooley0': 701, 'Hedman0': 646, 'Madigan0': 181, 'Parr0': 524, 'Campbell0': 401, 'Mullens0': 569, 'Charters0': 363, 'Jonsson0': 477, 'Ostby1': 54, 'Wilhelms0': 551, 'Williams1': 145, 'Williams0': 18, 'Maenpaa0': 221, 'Stone0': 662, 'Lennon1': 46, 'Olsson0': 254, 'Harknett0': 214, 'Horgan0': 508, 'Hart0': 353, 'Hart2': 283, 'Ilett0': 82, 'Baumann0': 156, 'Youseff0': 185, 'Reeves0': 240, 'Thomas1': 645, 'Jensen0': 527, 'Jensen1': 584, 'Cribb1': 150, 'Ross0': 486, 'Andersen-Jensen1': 176, 'Gilnagh0': 146, 'Moran0': 6, 'Coelho0': 123, 'Razi0': 679, 'Reuchlin0': 660, 'Kiernan1': 196, 'Partner0': 295, 'Jermyn0': 322, 'Salkjelsvik0': 103, 'Lahtinen2': 281, 'Drew2': 358, 'Hosono0': 260, 'Danoff0': 289, 'van Billiard2': 143, 'Bracken0': 203, 'Coutts2': 305, 'Connors0': 113, 'Jenkin0': 69, 'Hodges0': 586, 'Vovk0': 442, 'Backstrom1': 188, 'Devaney0': 44, 'Backstrom3': 83, 'Barbara1': 317, \"O'Connell0\": 520, 'Vande Velde0': 608, 'Reynaldo0': 380, 'Isham0': 163, \"O'Brien0\": 463, \"O'Brien1\": 170, 'Giglio0': 130, 'Ponesell0': 644, 'Nicola-Yarred1': 39, 'Angle1': 439, 'Mellinger1': 246, 'Newell1': 197, 'Lang0': 431, 'Davidson1': 549, 'Richard0': 127, 'Ball0': 293, 'Drazenoic0': 122, 'Robins1': 124, 'Richards5': 376, 'Nosworthy0': 51, 'Moore0': 116, 'Newsom2': 128, 'Sjoblom0': 635, 'Zimmerman0': 364, 'Kenyon1': 392, 'Fynney0': 21, 'Laroche3': 43, 'Renouf1': 409, 'Silvey1': 375, 'Christy2': 484, 'Renouf3': 588, 'Theobald0': 612, 'Pekoniemi0': 112, 'Bateman0': 140, 'Spencer1': 31, 'Allison3': 269, 'Cohen0': 186, 'Dimic0': 306, 'Bourke2': 172, 'Farthing0': 447, 'Hamalainen2': 224, 'Karaic0': 504, 'Pettersson0': 648, 'Landergren0': 328, 'Willey0': 532, 'Braund1': 1, 'Slemen0': 652, 'Jalsevac0': 390, 'Harder1': 324, 'Elsbury0': 494, 'Carrau0': 81, 'Sutehall0': 697, 'Sawyer0': 554, 'Vander Planke1': 19, 'Peters0': 557, 'Vander Planke2': 38, 'Dahlberg0': 695, 'Sharp0': 462, 'Sirota0': 667, 'Barkworth0': 521, 'Mack0': 623, 'Sjostedt0': 212, 'Slabenoff0': 499, 'Dodge2': 382, 'Mineff0': 266, 'Hocking3': 449, 'Hickman2': 115, 'Perkin0': 194, 'Stewart0': 64, 'Giles1': 681, 'Lindblom0': 250, 'Meek0': 357, 'Morrow0': 470, 'Newell2': 539, 'Beavan0': 326, 'Balkic0': 688, 'van Melkebeke0': 687, 'Foreman0': 387, 'Hampe0': 378, 'Birkeland0': 351, 'Mockler0': 315, 'Millet0': 391, 'Peter2': 120, 'Yasbeck1': 512, 'Osman0': 642, 'Buss0': 337, 'Sunderland0': 202, 'Lefebre4': 162, 'Bryhl1': 590, 'McGovern0': 314, 'Nye0': 66, 'Davis0': 525, 'Aubart0': 323, 'Spedden2': 287, 'Harris0': 201, 'Harris1': 62, 'McGough0': 433, 'Hewlett0': 16, 'Cann0': 37, 'Mallet2': 656, 'Kirkland0': 517, 'Ridsdale0': 446, 'Connaghton0': 605, 'Stoytcheff0': 475, 'Hippach1': 294, 'Dowdell0': 77, 'Klasen2': 161, 'Rothes0': 613, \"O'Connor0\": 394, 'Ivanoff0': 597, 'Clarke1': 367, 'Scanlan0': 403, 'Troupiansky0': 595, 'Reed0': 227, 'Rintamaki0': 492, 'Chambers1': 587, 'Haas0': 265, 'Greenfield1': 94, 'Jansson0': 341, 'Gheorgheff0': 362, 'Mitchell0': 550, 'Bonnell0': 12, 'Murdlin0': 491, 'Lester0': 651, 'Van Impe2': 361, 'Simonius-Blumer0': 531, 'Torber0': 501, 'Gillespie0': 585, 'Naidenoff0': 259, 'McNamee1': 601, 'de Pelsmaeker0': 255, 'Quick2': 429, 'Byles0': 139, 'Sage10': 149, 'Frolicher-Stehli2': 489, 'Peduzzi0': 389, 'Chibnall1': 155, 'Hakkarainen1': 133, 'Watt0': 151, 'Sirayanian0': 60, 'Leonard0': 165, 'Bissette0': 243, 'Adahl0': 319, 'LeRoy0': 452, 'Tomlin0': 653, 'Kimball1': 513, 'Gavey0': 511, 'Mionoff0': 102, 'Farrell0': 445, 'Goodwin7': 59, 'Strom1': 187, 'Ali0': 192, 'Funk0': 313, 'Van der hoef0': 158, 'Roebling0': 686, 'Simmons0': 473, 'Asim0': 318, 'Rosblom2': 231, 'Lovell0': 209, 'Phillips0': 368, 'Leinonen0': 526, 'Markoff0': 676, 'Levy0': 264, 'Frauenthal2': 540, 'Frauenthal1': 296, 'Baclini3': 384, 'Johannesen-Bratthammer0': 381, 'Jonkoff0': 609, 'Williams-Lambert0': 308, 'Serepeca0': 672, 'Longley0': 518, \"O'Dwyer0\": 28, 'Kelly0': 271, 'Douglas1': 457, 'Crosby2': 455, 'Niskanen0': 345, 'Collander0': 301, 'Gee0': 397, 'Tikkanen0': 334, 'McKane0': 342, 'Molson0': 418, 'Bostandyeff0': 519, 'Ayoub0': 631, 'Rogers0': 45, 'Chip0': 668, 'Cook0': 546, 'Stead0': 229, 'Soholt0': 580, 'Moraweck0': 285, 'Bidois0': 332, 'Taylor1': 547, 'Bradley0': 430, 'Fischer0': 561, 'Barber0': 262, 'Albimona0': 189, 'Hale0': 164, 'Flynn0': 369, 'Sadlier0': 338, 'Keane0': 274, 'Young0': 291, 'Lindahl0': 223, 'Rugg0': 56, 'Johnston3': 633, 'Petterson1': 379, 'Pavlovic0': 440, 'Sivic0': 471, 'Hirvonen1': 411, 'Allum0': 664, 'Harrington0': 500, 'Windelov0': 417, 'Lesurer0': 596, 'Pain0': 343, 'Lievens0': 622, 'Fox0': 303, 'Lewy0': 267, 'McMahon0': 118, 'Greenberg0': 579, 'Heikkinen0': 3, 'Louch1': 373, 'Chapman1': 495, 'Berriman0': 594, 'Moussa0': 321, 'Ryan0': 438, 'Madill1': 562, 'Duran y More1': 685, 'Bengtsson0': 152, 'Harper1': 52, 'Kilgannon0': 629, 'Badt0': 541, 'Salonen0': 448, 'Guggenheim0': 636, 'Widegren0': 349, 'Brewe0': 619, 'Sivola0': 159, 'Nirva0': 615, 'Rice5': 17, 'Andrews0': 647, 'Andrews1': 249, 'Collyer2': 216, 'Strandberg0': 407, 'Colley0': 542, 'Nicholson0': 458, 'Chaffee1': 89, 'Gaskell0': 637, 'Wells2': 606, 'Leeni0': 464, 'Todoroff0': 29, 'Adams0': 346, 'Pears1': 141, 'Sagesser0': 528, 'Carr0': 190, 'McGowan0': 23, 'Hansen0': 514, 'Hansen1': 574, 'Karun1': 564, 'Rush0': 479, 'Matthews0': 360, 'Laitinen0': 427, 'Padro y Manent0': 459, 'Andrew0': 135, 'Vander Cruyssen0': 689, 'Jacobsohn1': 199, 'Lurette0': 178, 'Jacobsohn3': 498, 'Hegarty0': 536}\n"
     ]
    }
   ],
   "source": [
    "# First, we'll add titles to the test set.\n",
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test[\"Title\"] = titles\n",
    "# Check the counts of each unique title.\n",
    "print(pandas.value_counts(titanic_test[\"Title\"]))\n",
    "\n",
    "# Now, we add the family size column.\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]\n",
    "\n",
    "# Now we can add family ids.\n",
    "# We'll use the same ids that we did earlier.\n",
    "print(family_id_mapping)\n",
    "\n",
    "family_ids = titanic_test.apply(get_family_id, axis=1)\n",
    "family_ids[titanic_test[\"FamilySize\"] < 3] = -1\n",
    "titanic_test[\"FamilyId\"] = family_ids\n",
    "\n",
    "titanic_test[\"NameLength\"] = titanic_test[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "\n",
    "predictions[predictions <= .5] = 0\n",
    "predictions[predictions > .5] = 1\n",
    "predictions = predictions.astype(int)\n",
    "\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"kaggleFeatures.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Kaggle rating improved significantly to 0.79904 by classifying early on the features I want to focus on and applying gradient boosting and logistic regression on the titles that affect the result more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe that there is definetely a lot of different things one can think about to improve the model even more. As I explored in the model_iteration_1 notebook, I created a binary parameter that specified whether or not a person was a child or not by defining a specific threshold that determined the age barrier. Including this idea into a future iteration of the model would definetely improve the prediction. Ideas for future exploration could involve the use of more parameters, such as rankings based on particular and more specific age groups. Finally, I would love to devote more time in making more appropriate helper functions, as well as understand in more detail how the selectKBest method works, as right now it seems to be treated like a black box."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
